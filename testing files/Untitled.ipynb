{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import DQN_mpi as DQNagent\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "simulators = size-1\n",
    "\n",
    "simulations = 500\n",
    "rep_interval = 25\n",
    "repetitions = int(simulations / (rep_interval*simulators))\n",
    "rep_each = int(simulations / simulators)\n",
    "weights = None\n",
    "\n",
    "if rank == 0:\n",
    "    start_time = time.time()\n",
    "\n",
    "FILE_NAME = \"ann-weights.h5\"\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "if rank == 0:\n",
    "    agent = DQNagent.agent(state_size,action_size,gamma=0.999 , epsilon = 1.0, epsilon_min=0.001,epsilon_decay=0.95, learning_rate=0.001, batch_size=128)\n",
    "\n",
    "#first simulation to have training data\n",
    "if not rank == 0 :\n",
    "    agent = DQNagent.simulator(state_size,action_size , epsilon = 1.0, epsilon_min=0.001,epsilon_decay=0.95, batch_size=128)\n",
    "    for e in range(rep_interval):\n",
    "        state = env.reset()\n",
    "        state = agent.format_state(state)\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            scores = deque(maxlen=100)\n",
    "            mean_score = 0\n",
    "            action = agent.action(state)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            new_state = agent.format_state(new_state)\n",
    "            agent.remember(state, action, reward, new_state, done)\n",
    "            state= new_state\n",
    "            score += reward\n",
    "        scores.append(score)\n",
    "        mean_score = np.mean (scores)\n",
    "        #print episode results\n",
    "\n",
    "\n",
    "        agent.reduce_random()\n",
    "\n",
    "    print(\"episode: {}/{}, score: {}, e: {:.2}, mean_score: {}\"\n",
    "        .format(e, rep_each, score, agent.epsilon,mean_score))\n",
    "\n",
    "#First gather and save the data to start the iterative process\n",
    "data = comm.gather(agent.memory, root=0)\n",
    "if rank == 0:\n",
    "        for d in data[1:]:\n",
    "            agent.memory += d\n",
    "        data = [[]]*(size-1)\n",
    "        for i in range(1,size):\n",
    "            data[i-1] = comm.irecv(source=i, tag=12)\n",
    "\n",
    "else:\n",
    "    network = comm.irecv(source=0, tag=11)\n",
    "            \n",
    "\n",
    "for i in range(repetitions):\n",
    "    if rank == 0:\n",
    "        #load weights to send\n",
    "        w_model=agent.model.get_weights()\n",
    "        for i in range(1,size):\n",
    "            #chek if received new data\n",
    "            flag, d = data[i-1].test()\n",
    "                if not flag:\n",
    "                    #if not received we cancel to ask again for data\n",
    "                    data[i-1].cancel()\n",
    "                else:\n",
    "                    data[i-1] = comm.irecv(source=i, tag=12)\n",
    "                    comm.isend(w_model, dest=i, tag=11)\n",
    "                    agent.memory += d\n",
    "            \n",
    "        for e in range(rep_interval*simulators):\n",
    "            agent.replay()\n",
    "            agent.soft_update_target_network()\n",
    "        print('neuron trained')\n",
    "    else:\n",
    "        agent.memory=deque()\n",
    "        for e in range(rep_interval):\n",
    "            state = env.reset()\n",
    "            state = agent.format_state(state)\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                action = agent.action(state)\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                new_state = agent.format_state(new_state)\n",
    "                agent.remember(state, action, reward, new_state, done)\n",
    "                state= new_state\n",
    "                score += reward\n",
    "            scores.append(score)\n",
    "            mean_score = np.mean (scores)\n",
    "            agent.reduce_random()\n",
    "\n",
    "        print(\"episode: {}/{}, score: {}, e: {:.2}, mean_score: {}\"\n",
    "            .format(e+(i+1)*rep_interval, rep_each, score, agent.epsilon,mean_score))\n",
    "        flag, weights = network.test()\n",
    "        if not flag:\n",
    "            network.Cancel()\n",
    "        else:\n",
    "            comm.isend(agent.memory, dest=0, tag=12)\n",
    "            agent.memory=deque()\n",
    "            agent.model.set_weights(weights)\n",
    "            network = comm.irecv(source=0, tag=11)\n",
    "\n",
    "\n",
    "\n",
    "if rank == 0:\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('time to run:', elapsed_time )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([[3,2,4],[3,23,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
